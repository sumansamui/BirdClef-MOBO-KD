{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-13T13:00:02.831136Z",
     "iopub.status.busy": "2025-07-13T13:00:02.830918Z",
     "iopub.status.idle": "2025-07-13T13:00:16.927023Z",
     "shell.execute_reply": "2025-07-13T13:00:16.926069Z",
     "shell.execute_reply.started": "2025-07-13T13:00:02.831114Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras import optimizers\n",
    "import keras\n",
    "from functools import partial\n",
    "from math import exp\n",
    "from tensorflow.keras.utils import get_custom_objects\n",
    "from keras.layers import Activation\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from matplotlib.cm import get_cmap\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T13:02:12.854201Z",
     "iopub.status.busy": "2025-07-13T13:02:12.853483Z",
     "iopub.status.idle": "2025-07-13T13:02:23.179389Z",
     "shell.execute_reply": "2025-07-13T13:02:23.178763Z",
     "shell.execute_reply.started": "2025-07-13T13:02:12.854174Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "melspec_directory = '/kaggle/input/birdcall-melspec'\n",
    "import h5py\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "def load_from_hdf5(filename, directory):\n",
    "    \"\"\"Load data from HDF5 file\"\"\"\n",
    "    filepath = os.path.join(directory, filename)\n",
    "    with h5py.File(filepath, 'r') as hf:\n",
    "        data = {name: hf[name][:] for name in hf.keys()}\n",
    "\n",
    "    if 'classes' in data:\n",
    "        label_encoder = LabelEncoder()\n",
    "        label_encoder.classes_ = data['classes'].astype(str)\n",
    "        data['label_encoder'] = label_encoder\n",
    "\n",
    "    return data\n",
    "\n",
    "loaded_data = load_from_hdf5('mfcc_data.h5', melspec_directory)\n",
    "\n",
    "X = loaded_data['X_train']\n",
    "y = loaded_data['y_train']\n",
    "label_encoder = loaded_data['label_encoder']\n",
    "\n",
    "original_labels = label_encoder.inverse_transform(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T13:02:43.153682Z",
     "iopub.status.busy": "2025-07-13T13:02:43.153421Z",
     "iopub.status.idle": "2025-07-13T13:02:43.976577Z",
     "shell.execute_reply": "2025-07-13T13:02:43.975997Z",
     "shell.execute_reply.started": "2025-07-13T13:02:43.153661Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp,\n",
    "    y_temp,\n",
    "    test_size=0.5,\n",
    "    random_state=42,\n",
    "    stratify=y_temp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T13:02:46.975382Z",
     "iopub.status.busy": "2025-07-13T13:02:46.974710Z",
     "iopub.status.idle": "2025-07-13T13:02:46.980430Z",
     "shell.execute_reply": "2025-07-13T13:02:46.979655Z",
     "shell.execute_reply.started": "2025-07-13T13:02:46.975355Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ModelInspector:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.layer_dict = {layer.name: layer for layer in model.layers}\n",
    "\n",
    "    def get_layer_output(self, layer_name, data):\n",
    "\n",
    "        intermediate_model = tf.keras.Model(\n",
    "            inputs=self.model.input,\n",
    "            outputs=self.model.get_layer(layer_name).output\n",
    "        )\n",
    "        return intermediate_model.predict(data)\n",
    "\n",
    "    def list_layers(self):\n",
    "\n",
    "        for idx, layer in enumerate(self.model.layers):\n",
    "            print(f\"Layer {idx}: {layer.name} ({layer.__class__.__name__})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T13:02:49.942443Z",
     "iopub.status.busy": "2025-07-13T13:02:49.941994Z",
     "iopub.status.idle": "2025-07-13T13:02:52.907075Z",
     "shell.execute_reply": "2025-07-13T13:02:52.906476Z",
     "shell.execute_reply.started": "2025-07-13T13:02:49.942418Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_path = '/kaggle/input/teacher/keras/default/1/teacher.keras'\n",
    "teacher_model = load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T13:03:05.645745Z",
     "iopub.status.busy": "2025-07-13T13:03:05.645445Z",
     "iopub.status.idle": "2025-07-13T13:03:14.643697Z",
     "shell.execute_reply": "2025-07-13T13:03:14.643118Z",
     "shell.execute_reply.started": "2025-07-13T13:03:05.645724Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "y_pred = teacher_model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "test_loss, test_acc = teacher_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"\\nTest Accuracy: {test_acc:.4f}\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T13:03:33.711727Z",
     "iopub.status.busy": "2025-07-13T13:03:33.710924Z",
     "iopub.status.idle": "2025-07-13T13:03:33.715944Z",
     "shell.execute_reply": "2025-07-13T13:03:33.715111Z",
     "shell.execute_reply.started": "2025-07-13T13:03:33.711696Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "NCLASS=11\n",
    "input_shape = X_train.shape[1:]\n",
    "LEARNING_RATE= 0.002"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student model formation\n",
    "\n",
    "The pareto-optimal student networks found after optimization in Neural architectural search are recreated using the obtained information regarding their architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T13:03:35.911028Z",
     "iopub.status.busy": "2025-07-13T13:03:35.910671Z",
     "iopub.status.idle": "2025-07-13T13:03:35.987618Z",
     "shell.execute_reply": "2025-07-13T13:03:35.987059Z",
     "shell.execute_reply.started": "2025-07-13T13:03:35.911004Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_2d_cnn_model(residual, filters, kernel_size, fc_layers, use_bn, use_dropout, input_shape):\n",
    "\n",
    "    inputs = layers.Input(shape=input_shape, name=\"input_layer\")\n",
    "\n",
    "    # Initial Conv Block\n",
    "    x = layers.Conv2D(filters, kernel_size, padding='same')(inputs)\n",
    "    if use_bn:\n",
    "        x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU(name=\"relu1_1\")(x)\n",
    "    x = layers.Conv2D(filters, kernel_size, padding='same')(x)\n",
    "    if use_bn:\n",
    "        x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.MaxPooling2D((2,2), strides=(2,2), padding='same')(x)  # Changed to (2,2)\n",
    "\n",
    "\n",
    "    # Residual Block\n",
    "    for _ in range(residual - 1):\n",
    "        filters = filters*2\n",
    "        residual = layers.Conv2D(filters, (1,1), strides=(2,2), padding='same')(x)\n",
    "        x = layers.Conv2D(filters, kernel_size, padding='same')(x)\n",
    "        if use_bn:\n",
    "            x = layers.BatchNormalization()(x)\n",
    "        x = layers.ReLU()(x)\n",
    "        x = layers.Conv2D(filters, kernel_size, padding='same')(x)\n",
    "        if use_bn:\n",
    "            x = layers.BatchNormalization()(x)\n",
    "        x = layers.MaxPooling2D((2,2), strides=(2,2), padding='same')(x)\n",
    "        x = layers.add([x, residual])\n",
    "        x = layers.ReLU()(x)\n",
    "\n",
    "    # Final Feature Processing\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "    # Dense Layers\n",
    "    fc_layer_configs = {\n",
    "        4: [1024, 512, 256, 128],\n",
    "        3: [512, 256, 128],\n",
    "        2: [256, 128],\n",
    "        1: [128]\n",
    "        }\n",
    "\n",
    "    num_fc_layers = fc_layers  # Example: 4, 3, 2, or 1\n",
    "\n",
    "    if num_fc_layers in fc_layer_configs:\n",
    "        for i, neurons in enumerate(fc_layer_configs[num_fc_layers]):\n",
    "            x = layers.Dense(neurons, activation='relu')(x)\n",
    "            if use_dropout:\n",
    "                x = layers.Dropout(0.2)(x)\n",
    "\n",
    "\n",
    "    outputs = layers.Dense(NCLASS, activation='softmax', name=\"output_layer\")(x)\n",
    "\n",
    "    # Compile\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        #clipnorm=clipnorm,\n",
    "        #clipvalue=clipvalue\n",
    "    )\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "    #model.summary()\n",
    "    trainable_params = np.sum([tf.keras.backend.count_params(w) for w in model.trainable_weights])\n",
    "    non_trainable_params = np.sum([tf.keras.backend.count_params(w) for w in model.non_trainable_weights])\n",
    "    total_params = trainable_params + non_trainable_params\n",
    "\n",
    "    # Calculate size in MB (assuming float32 precision - 4 bytes per parameter)\n",
    "    param_size_mb = (total_params * 4) / (1024)  # bytes to MB conversion\n",
    "\n",
    "    print(f\"Model created with:\")\n",
    "    print(f\"- Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"- Non-trainable parameters: {non_trainable_params:,}\")\n",
    "    print(f\"- Total parameters: {total_params:,}\")\n",
    "    print(f\"- Estimated size: {param_size_mb:.2f} KB (assuming float32)\")\n",
    "\n",
    "    return model\n",
    "\n",
    "input_shape = X_train.shape[1:]\n",
    "student_model = create_2d_cnn_model(2, 32, (3, 3), 2, 0, 0, input_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T13:21:00.231534Z",
     "iopub.status.busy": "2025-07-13T13:21:00.230931Z",
     "iopub.status.idle": "2025-07-13T13:21:00.830606Z",
     "shell.execute_reply": "2025-07-13T13:21:00.829989Z",
     "shell.execute_reply.started": "2025-07-13T13:21:00.231511Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "model_configs = [\n",
    "\n",
    "    {'residual': 1, 'filters': 16, 'kernel_size': (3,3), 'fc_layers': 1, 'use_bn': 0, 'use_dropout': 0},\n",
    "    {'residual': 2, 'filters': 32, 'kernel_size': (3,3), 'fc_layers': 2, 'use_bn': 0, 'use_dropout': 0},\n",
    "    {'residual': 1, 'filters': 32, 'kernel_size': (5,5), 'fc_layers': 2, 'use_bn': 1, 'use_dropout': 0},\n",
    "    {'residual': 1, 'filters': 32, 'kernel_size': (3,3), 'fc_layers': 1, 'use_bn': 0, 'use_dropout': 1},\n",
    "    {'residual': 1, 'filters': 16, 'kernel_size': (5,5), 'fc_layers': 1, 'use_bn': 1, 'use_dropout': 1},\n",
    "    {'residual': 1, 'filters': 16, 'kernel_size': (5,5), 'fc_layers': 2, 'use_bn': 0, 'use_dropout': 1},\n",
    "    {'residual': 2, 'filters': 64, 'kernel_size': (3,3), 'fc_layers': 1, 'use_bn': 1, 'use_dropout': 1},\n",
    "    {'residual': 1, 'filters': 32, 'kernel_size': (5,5), 'fc_layers': 2, 'use_bn': 0, 'use_dropout': 1},\n",
    "    {'residual': 2, 'filters': 32, 'kernel_size': (5,5), 'fc_layers': 2, 'use_bn': 0, 'use_dropout': 0},\n",
    "    {'residual': 2, 'filters': 64, 'kernel_size': (3,3), 'fc_layers': 1, 'use_bn': 0, 'use_dropout': 0},\n",
    "    {'residual': 2, 'filters': 32, 'kernel_size': (3,3), 'fc_layers': 1, 'use_bn': 0, 'use_dropout': 1},\n",
    "    {'residual': 2, 'filters': 64, 'kernel_size': (5,5), 'fc_layers': 1, 'use_bn': 1, 'use_dropout': 1},\n",
    "]\n",
    "\n",
    "student_models = []\n",
    "for idx, config in enumerate(model_configs, 1):\n",
    "    print(f\"\\nCreating student_model_{idx}\")\n",
    "    model = create_2d_cnn_model(\n",
    "        input_shape=input_shape,\n",
    "        **config\n",
    "    )\n",
    "    student_models.append(model)\n",
    "\n",
    "print(f\"\\nTotal models created: {len(student_models)}\")\n",
    "print(f\"First model: {student_models[0]}\")\n",
    "print(f\"Last model: {student_models[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T13:21:10.996798Z",
     "iopub.status.busy": "2025-07-13T13:21:10.996088Z",
     "iopub.status.idle": "2025-07-13T13:21:11.001242Z",
     "shell.execute_reply": "2025-07-13T13:21:11.000592Z",
     "shell.execute_reply.started": "2025-07-13T13:21:10.996776Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class SaveBestStudentModel(keras.callbacks.Callback):\n",
    "    def __init__(self, save_path):\n",
    "        super().__init__()\n",
    "        self.save_path = save_path\n",
    "        self.best_val_loss = float(\"inf\")\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        val_student_loss = logs.get(\"val_student_loss\")\n",
    "        if val_student_loss is not None and val_student_loss < self.best_val_loss:\n",
    "            self.best_val_loss = val_student_loss\n",
    "            print(f\"\\nSaving best student model with val_student_loss: {val_student_loss:.4f}\")\n",
    "            self.model.student.save(self.save_path)  # Only student model is saved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teacher-student knowledge distillation\n",
    "\n",
    "The overall loss function using cross entropy loss from hard labels and KL-Divergence loss between soft labels of teacher and student models are defined.\n",
    "The forward pass and callbacks based on validation_loss is implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T13:21:13.668983Z",
     "iopub.status.busy": "2025-07-13T13:21:13.668499Z",
     "iopub.status.idle": "2025-07-13T13:21:13.678963Z",
     "shell.execute_reply": "2025-07-13T13:21:13.678013Z",
     "shell.execute_reply.started": "2025-07-13T13:21:13.668960Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Distiller(keras.Model):\n",
    "    def __init__(self, student, teacher):\n",
    "        super().__init__()\n",
    "        self.teacher = teacher\n",
    "        self.student = student\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Forward pass of the student model\n",
    "        return self.student(inputs)\n",
    "\n",
    "    def compile(\n",
    "        self,\n",
    "        optimizer,\n",
    "        metrics,\n",
    "        student_loss_fn,\n",
    "        distillation_loss_fn,\n",
    "        alpha=0.1,\n",
    "        temperature=3,\n",
    "    ):\n",
    "        \"\"\" Configure the distiller.\n",
    "\n",
    "        Args:\n",
    "            optimizer: Keras optimizer for the student weights\n",
    "            metrics: Keras metrics for evaluation\n",
    "            student_loss_fn: Loss function of difference between student\n",
    "                predictions and ground-truth\n",
    "            distillation_loss_fn: Loss function of difference between soft\n",
    "                student predictions and soft teacher predictions\n",
    "            alpha: weight to student_loss_fn and 1-alpha to distillation_loss_fn\n",
    "            temperature: Temperature for softening probability distributions.\n",
    "                Larger temperature gives softer distributions.\n",
    "        \"\"\"\n",
    "        super().compile(optimizer=optimizer, metrics=metrics)\n",
    "        self.student_loss_fn = student_loss_fn\n",
    "        self.distillation_loss_fn = distillation_loss_fn\n",
    "        self.alpha = alpha\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # Unpack data\n",
    "        x, y = data\n",
    "\n",
    "        # Forward pass of teacher\n",
    "        teacher_predictions = self.teacher(x, training=False)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass of student\n",
    "            student_predictions = self.student(x, training=True)\n",
    "\n",
    "            # Compute losses\n",
    "            student_loss = self.student_loss_fn(y, student_predictions)\n",
    "\n",
    "\n",
    "            distillation_loss = (\n",
    "                self.distillation_loss_fn(\n",
    "                    tf.nn.softmax(teacher_predictions / self.temperature, axis=1),\n",
    "                    tf.nn.softmax(student_predictions / self.temperature, axis=1),\n",
    "                )\n",
    "                * self.temperature**2\n",
    "            )\n",
    "\n",
    "            loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.student.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "        # Update the metrics configured in `compile()`.\n",
    "        self.compiled_metrics.update_state(y, student_predictions)\n",
    "\n",
    "        # Return a dict of performance\n",
    "        results = {m.name: m.result() for m in self.metrics}\n",
    "        results.update({\n",
    "                        \"student_loss\": student_loss,\n",
    "                        \"distillation_loss\": distillation_loss,\n",
    "                        \"loss\": loss })\n",
    "\n",
    "        return results\n",
    "\n",
    "    def test_step(self, data):\n",
    "        # Unpack the data\n",
    "        x, y = data\n",
    "\n",
    "        # Compute predictions\n",
    "        y_prediction = self.student(x, training=False)\n",
    "\n",
    "        # Calculate the loss\n",
    "        student_loss = self.student_loss_fn(y, y_prediction)\n",
    "\n",
    "        # Update the metrics.\n",
    "        self.compiled_metrics.update_state(y, y_prediction)\n",
    "\n",
    "        # Return a dict of performance\n",
    "        results = {m.name: m.result() for m in self.metrics}\n",
    "        results.update({\"student_loss\": student_loss, \"loss\": student_loss})\n",
    "        return results\n",
    "    def predict_step(self, data):\n",
    "        x = data[0] if isinstance(data, (tuple, list)) else data\n",
    "        return self.student(x, training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T13:22:24.281220Z",
     "iopub.status.busy": "2025-07-13T13:22:24.280940Z",
     "iopub.status.idle": "2025-07-13T13:23:30.048080Z",
     "shell.execute_reply": "2025-07-13T13:23:30.047385Z",
     "shell.execute_reply.started": "2025-07-13T13:22:24.281199Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "student_best_path = \"/kaggle/working/student_best.keras\"\n",
    "SAVED_MODEL_PATH = \"/kaggle/working/student_last.keras\"\n",
    "\n",
    "save_best_student = SaveBestStudentModel(student_best_path)\n",
    "\n",
    "# Compile the distiller\n",
    "distiller = Distiller(student=student_models[0], teacher=teacher_model)\n",
    "distiller.compile(\n",
    "    optimizer=Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.99),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    student_loss_fn=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    distillation_loss_fn=keras.losses.KLDivergence(),\n",
    "    alpha=1,\n",
    "    temperature=5\n",
    ")\n",
    "\n",
    "# Train the distiller\n",
    "history_distill = distiller.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=5,\n",
    "    batch_size=64,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[save_best_student],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T13:23:33.925543Z",
     "iopub.status.busy": "2025-07-13T13:23:33.925299Z",
     "iopub.status.idle": "2025-07-13T13:23:35.990522Z",
     "shell.execute_reply": "2025-07-13T13:23:35.989894Z",
     "shell.execute_reply.started": "2025-07-13T13:23:33.925527Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "student0_best= load_model(student_best_path)\n",
    "test_loss, test_acc = student0_best.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"\\nTest Accuracy: {test_acc:.4f}\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training without teacher influence\n",
    "\n",
    "All student models are trained without any help from teacher (alpha=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T13:39:07.718440Z",
     "iopub.status.busy": "2025-07-13T13:39:07.717500Z",
     "iopub.status.idle": "2025-07-13T13:41:12.554261Z",
     "shell.execute_reply": "2025-07-13T13:41:12.553307Z",
     "shell.execute_reply.started": "2025-07-13T13:39:07.718414Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "student_models_trained = []\n",
    "histories = []\n",
    "\n",
    "for i in range(10):\n",
    "    student_best_path = f\"/kaggle/working/student_best_{i}.keras\"\n",
    "    save_best_student = SaveBestStudentModel(student_best_path)\n",
    "    \n",
    "    distiller = Distiller(student=student_models[i], teacher=teacher_model)\n",
    "    distiller.compile(\n",
    "        optimizer=Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.99),\n",
    "        metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "        student_loss_fn=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        distillation_loss_fn=keras.losses.KLDivergence(),\n",
    "        alpha=1,\n",
    "        temperature=1\n",
    "    )\n",
    "    \n",
    "    history = distiller.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        epochs=25,\n",
    "        batch_size=64,\n",
    "        validation_data=(X_val, y_val),\n",
    "        callbacks=[save_best_student],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    histories.append(history)\n",
    "    student_models_trained.append(student_best_path)\n",
    "\n",
    "val_accuracies = []\n",
    "test_accuracies = []\n",
    "val_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for i, model_path in enumerate(student_models_trained):\n",
    "    student_model = load_model(model_path)\n",
    "    \n",
    "    val_loss, val_acc = student_model.evaluate(X_val, y_val, verbose=0)\n",
    "    test_loss, test_acc = student_model.evaluate(X_test, y_test, verbose=0)\n",
    "    \n",
    "    val_accuracies.append(val_acc)\n",
    "    test_accuracies.append(test_acc)\n",
    "    val_losses.append(val_loss)\n",
    "    test_losses.append(test_loss)\n",
    "print(\"\\nIndividual Model Performances:\")\n",
    "for i in range(10):\n",
    "    print(f\"Model {i+1}: Val Acc={val_accuracies[i]:.4f}, Test Acc={test_accuracies[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ablation study for Teacher influenced training\n",
    "\n",
    "The hyperparameters alpha and temperature are varied to train the best student models with influence of the teacher soft labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperatures = [3, 5, 7, 10, 12, 14]\n",
    "alphas = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "all_results = []\n",
    "\n",
    "for student_idx in range(10):\n",
    "    print(f\"\\n=== Training Student Model {student_idx+1}/10 ===\")\n",
    "    student_best_metrics = {'val_acc': 0, 'params': None, 'model_path': None}\n",
    "    \n",
    "    # Random regularization helps avoid overfitting to specific hyperparameter combinations while maintaining computational efficiency\n",
    "    combinations = [(t, a) for t in temperatures for a in alphas]\n",
    "    selected_combinations = random.sample(combinations, len(combinations)//2)\n",
    "    \n",
    "    for temp, alpha in selected_combinations:\n",
    "        print(f\"\\nTraining with temperature={temp}, alpha={alpha}\")\n",
    "            \n",
    "        model_path = f\"/kaggle/working/student_{student_idx}_taught_temp{temp}_alpha{alpha}.keras\"\n",
    "        save_best = SaveBestStudentModel(model_path)\n",
    "        \n",
    "        distiller = Distiller(student=student_models[student_idx], teacher=teacher_model)\n",
    "        distiller.compile(\n",
    "            optimizer=Adam(learning_rate=0.002, beta_1=0.9, beta_2=0.99),\n",
    "            metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "            student_loss_fn=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "            distillation_loss_fn=keras.losses.KLDivergence(),\n",
    "            alpha=alpha,\n",
    "            temperature=temp\n",
    "        )\n",
    "        \n",
    "        history = distiller.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            epochs=25,\n",
    "            batch_size=64,\n",
    "            validation_data=(X_val, y_val),\n",
    "            callbacks=[save_best],\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        best_model = load_model(model_path)\n",
    "        val_loss, val_acc = best_model.evaluate(X_val, y_val, verbose=0)\n",
    "        \n",
    "        if val_acc > student_best_metrics['val_acc']:\n",
    "            student_best_metrics = {\n",
    "                'val_acc': val_acc,\n",
    "                'params': {'temperature': temp, 'alpha': alpha},\n",
    "                'model_path': model_path,\n",
    "                'student_idx': student_idx\n",
    "            }\n",
    "        \n",
    "        print(f\"Val Acc: {val_acc:.4f} | Temp: {temp} | Alpha: {alpha}\")\n",
    "    \n",
    "    all_results.append(student_best_metrics)\n",
    "\n",
    "print(\"\\n=== Best Performing Models ===\")\n",
    "for result in all_results:\n",
    "    print(f\"\\nStudent {result['student_idx']+1}:\")\n",
    "    print(f\"Validation Accuracy: {result['val_acc']:.4f}\")\n",
    "    print(f\"Temperature: {result['params']['temperature']}\")\n",
    "    print(f\"Alpha: {result['params']['alpha']}\")\n",
    "    print(f\"Model Path: {result['model_path']}\")\n",
    "\n",
    "print(\"\\n=== Final Evaluation on Test Set ===\")\n",
    "test_results = []\n",
    "for result in all_results:\n",
    "    best_model = load_model(result['model_path'])\n",
    "    test_loss, test_acc = best_model.evaluate(X_test, y_test, verbose=0)\n",
    "    test_results.append(test_acc)\n",
    "    print(f\"\\nStudent {result['student_idx']+1}:\")\n",
    "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 11893428,
     "sourceId": 91716,
     "sourceType": "competition"
    },
    {
     "datasetId": 7471162,
     "sourceId": 11886840,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 351683,
     "modelInstanceId": 330818,
     "sourceId": 404753,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
